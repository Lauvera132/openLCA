{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c359fb",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This program performs analysis and visualization of openLCA results from an LCIA calculation, starting with an excel file export. This program assumes that you've already compiled an excel file of the unit processes exported from your openLCA database, with ISIC categories assigned to each process. This excel file should be named \"database_process_summary_added.xlsx\" with the corresponding file path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1309a",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e31a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5d3489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:17: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:17: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\laura\\AppData\\Local\\Temp\\ipykernel_34388\\3916949549.py:17: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  output_excel_file_path = r'C:\\Users\\laura\\OneDrive\\Documents\\UT_graduate\\WEG_graduate_research\\Hydrogen_fugitive_emissions_natural_h2\\openLCA_ecoinvent\\openLCA' + f'\\{product_system_name}_data_export.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Set up variables\n",
    "# Change these variables to match your data\n",
    "\n",
    "impact_category_name = \"climate change - global warming potential (GWP100) (Nitrous Oxide, Methane, Carbon Dioxide)\"\n",
    "product_system_name = \"GeoH2 (85% mol H2; H2 Average Fugitive; CH4 Capture)\"\n",
    "\n",
    "\n",
    "# LCA results file path and sheet names\n",
    "lca_results_file_path = r'C:\\Users\\laura\\OneDrive\\Documents\\UT_graduate\\WEG_graduate_research\\Hydrogen_fugitive_emissions_natural_h2\\openLCA_ecoinvent\\Hydrogen_gas__at_processing__production_mixture__to_consumer__kg___US__85__mol_H2____H2_Average___CH4_Capture.xlsx'\n",
    "direct_impact_sheet_name = 'Direct impact contributions' #sheet name in the LCA results file\n",
    "impact_by_flow_sheet_name = 'Impact contributions by flow' #sheet name in the LCA results file\n",
    "\n",
    "# Database process summary file path\n",
    "database_process_summary_file_path = r'C:\\Users\\laura\\OneDrive\\Documents\\UT_graduate\\WEG_graduate_research\\Hydrogen_fugitive_emissions_natural_h2\\openLCA_ecoinvent\\openLCA\\database_process_summary_added.xlsx'\n",
    "\n",
    "# Define the output Excel file path\n",
    "output_excel_file_path = r'C:\\Users\\laura\\OneDrive\\Documents\\UT_graduate\\WEG_graduate_research\\Hydrogen_fugitive_emissions_natural_h2\\openLCA_ecoinvent\\openLCA' + f'\\{product_system_name}_data_export.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d65bd4",
   "metadata": {},
   "source": [
    "# Read in LCA Results Excel File\n",
    "Read in LCA results impact by unit process and by flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b928d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the specified worksheet into a DataFrame\n",
    "df_process_impact = pd.read_excel(lca_results_file_path, sheet_name=direct_impact_sheet_name)\n",
    "df_flows_impact = pd.read_excel(lca_results_file_path, sheet_name=impact_by_flow_sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab1a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first column if it contains only empty values\n",
    "if df_process_impact.iloc[:, 0].isnull().all():\n",
    "    df_process_impact.drop(df_process_impact.columns[0], axis=1, inplace=True)\n",
    "if df_flows_impact.iloc[:, 0].isnull().all():\n",
    "    df_flows_impact.drop(df_flows_impact.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Transpose the DataFrame\n",
    "df_process_impact_transposed = df_process_impact.transpose()\n",
    "df_flows_impact_transposed = df_flows_impact.transpose()\n",
    "\n",
    "# Reset the index of the transposed DataFrame\n",
    "df_process_impact_transposed.reset_index(drop=True, inplace=True)\n",
    "df_flows_impact_transposed.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42484c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process column index: 7\n",
      "Flows column index: 9\n",
      "Impact category UUID: f1701a93-f252-4082-92f8-390ad9fd9bae\n",
      "Impact category reference unit: kg CO2-Eq\n"
     ]
    }
   ],
   "source": [
    "# Find the column index of the target impact category in the transposed DataFrames\n",
    "process_column_index = df_process_impact_transposed.columns[df_process_impact_transposed.iloc[1] == impact_category_name].tolist()\n",
    "flows_column_index = df_flows_impact_transposed.columns[df_flows_impact_transposed.iloc[1] == impact_category_name].tolist()\n",
    "\n",
    "# If the column is found, retrieve its index; otherwise, return None\n",
    "process_column_index = process_column_index[0] if process_column_index else None\n",
    "flows_column_index = flows_column_index[0] if flows_column_index else None\n",
    "print(f\"Process column index: {process_column_index}\")\n",
    "print(f\"Flows column index: {flows_column_index}\")\n",
    "\n",
    "# Create a copy of transposed DataFrames with only specified columns\n",
    "process_columns_to_keep = [0, 1, 2, process_column_index]\n",
    "flows_columns_to_keep = [0, 1, 2, 3, 4, flows_column_index]\n",
    "df_process_impact_filtered = df_process_impact_transposed.iloc[:, process_columns_to_keep]\n",
    "df_flows_impact_filtered = df_flows_impact_transposed.iloc[:, flows_columns_to_keep]\n",
    "\n",
    "impact_category_uuid = df_process_impact_filtered.iloc[0, 3]\n",
    "impact_category_reference_unit = df_process_impact_filtered.iloc[2, 3]\n",
    "print(f\"Impact category UUID: {impact_category_uuid}\")\n",
    "print(f\"Impact category reference unit: {impact_category_reference_unit}\")\n",
    "\n",
    "# Remove unnecessary rows and rename the columns of the filtered DataFrames\n",
    "df_process_impact_filtered = df_process_impact_filtered.iloc[4:].reset_index(drop=True)\n",
    "df_process_impact_filtered.columns = ['Process_UUID', 'Process_Name', 'Location', 'GWP100_Impact_Value_kg_CO2eq']\n",
    "df_flows_impact_filtered = df_flows_impact_filtered.iloc[4:].reset_index(drop=True)\n",
    "df_flows_impact_filtered.columns = ['Flow_UUID', 'Flow_Name', 'Flow_Category', 'Flow_Sub-category', 'Flow_Unit', 'GWP100_Impact_Value_kg_CO2eq']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a48dd",
   "metadata": {},
   "source": [
    "# Read in OpenLCA Database Process Summary Excel File with ISIC Classifications\n",
    "Read in unique list of unit processes in openLCA database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca6cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the database process summary file\n",
    "database_process_df = pd.read_excel(database_process_summary_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e2aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split column \"ISIC_Category\" into multiple columns using \"/\" as the delimiter\n",
    "split_columns = database_process_df['ISIC_Category'].str.split('/', expand=True)\n",
    "\n",
    "# Rename the new columns for clarity\n",
    "split_columns.columns = ['ISIC_Category_Section', 'ISIC_Category_Division', 'ISIC_Category_Group', 'ISIC_Category_Class']\n",
    "\n",
    "# Add the split columns as additional columns to database_process_df\n",
    "database_process_df = pd.concat([database_process_df, split_columns], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19433fba",
   "metadata": {},
   "source": [
    "# Merge Database Process Summary DataFrame to LCA results Process Impact DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a79a6702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the database process summary DataFrame with the filtered process impact DataFrame\n",
    "merged_process_df = pd.merge(df_process_impact_filtered, database_process_df, on='Process_UUID', how='left')\n",
    "\n",
    "# Check if the \"Process_Name\" column exists in both DataFrames after the merge\n",
    "if 'Process_Name_x' in merged_process_df.columns and 'Process_Name_y' in merged_process_df.columns:\n",
    "    # Check if the \"Process_Name\" columns match in both DataFrames\n",
    "    if (merged_process_df['Process_Name_x'] == merged_process_df['Process_Name_y']).all():\n",
    "        # Drop one of the \"Process_Name\" columns and rename the other to \"Process_Name\"\n",
    "        merged_process_df.drop(columns=['Process_Name_y'], inplace=True)\n",
    "        merged_process_df.rename(columns={'Process_Name_x': 'Process_Name'}, inplace=True)\n",
    "    else:\n",
    "        print(\"Warning: Process_Name columns do not match completely.\")\n",
    "else:\n",
    "    print(\"Error: Process_Name columns are missing in the merged DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d64988",
   "metadata": {},
   "source": [
    "# Analyze DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8283158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the GWP100 impact value to numeric, forcing errors to NaN\n",
    "merged_process_df['GWP100_Impact_Value_kg_CO2eq'] = pd.to_numeric(merged_process_df['GWP100_Impact_Value_kg_CO2eq'], errors='coerce')\n",
    "df_process_impact_filtered['GWP100_Impact_Value_kg_CO2eq'] = pd.to_numeric(df_process_impact_filtered['GWP100_Impact_Value_kg_CO2eq'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in the GWP100 impact value column\n",
    "merged_process_df.dropna(subset=['GWP100_Impact_Value_kg_CO2eq'], inplace=True)\n",
    "df_flows_impact_filtered.dropna(subset=['GWP100_Impact_Value_kg_CO2eq'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2648439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by \"Process_Name\" and sum the GWP100 impact values\n",
    "grouped_process_df = merged_process_df.groupby('Process_Name')['GWP100_Impact_Value_kg_CO2eq'].sum().reset_index()\n",
    "grouped_process_df.sort_values(by='GWP100_Impact_Value_kg_CO2eq', ascending=False, inplace=True)\n",
    "\n",
    "# Group the data by \"ISIC_Category_Section\" and sum the GWP100 impact values\n",
    "grouped_isic_df = merged_process_df.groupby('ISIC_Category_Section')['GWP100_Impact_Value_kg_CO2eq'].sum().reset_index()\n",
    "grouped_isic_df.sort_values(by='GWP100_Impact_Value_kg_CO2eq', ascending=False, inplace=True)\n",
    "\n",
    "# Group the data by \"Flow_Name\" and sum the GWP100 impact values\n",
    "grouped_flows_df = df_flows_impact_filtered.groupby('Flow_Name')['GWP100_Impact_Value_kg_CO2eq'].sum().reset_index()\n",
    "grouped_flows_df.sort_values(by='GWP100_Impact_Value_kg_CO2eq', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c299193",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xlsxwriter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Write the grouped DataFrames to separate sheets in the Excel file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_excel_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxlsxwriter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m      3\u001b[0m     grouped_process_df\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGrouped_Process\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     grouped_isic_df\u001b[38;5;241m.\u001b[39mto_excel(writer, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGrouped_ISIC\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\laura\\miniconda3\\Lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py:197\u001b[0m, in \u001b[0;36mXlsxWriter.__init__\u001b[1;34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m WriteExcelBuffer \u001b[38;5;241m|\u001b[39m ExcelWriter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# Use the xlsxwriter module as the Excel writer.\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxlsxwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[0;32m    199\u001b[0m     engine_kwargs \u001b[38;5;241m=\u001b[39m combine_kwargs(engine_kwargs, kwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xlsxwriter'"
     ]
    }
   ],
   "source": [
    "# Write the grouped DataFrames to separate sheets in the Excel file\n",
    "with pd.ExcelWriter(output_excel_file_path, engine='xlsxwriter') as writer:\n",
    "    grouped_process_df.to_excel(writer, sheet_name='Grouped_Process', index=False)\n",
    "    grouped_isic_df.to_excel(writer, sheet_name='Grouped_ISIC', index=False)\n",
    "    grouped_flows_df.to_excel(writer, sheet_name='Grouped_Flows', index=False)\n",
    "\n",
    "print(f\"Grouped data has been exported to {output_excel_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
